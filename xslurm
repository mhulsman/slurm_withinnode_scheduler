#!/usr/bin/env python
import curses, curses.textpad, curses.ascii
import os
import subprocess, sys, getopt, time,shlex
from subprocess import Popen
import thread
import threading
from collections import defaultdict, OrderedDict

from SimpleXMLRPCServer import SimpleXMLRPCServer
import random
import psutil
import numpy
import getpass

import xslurm_shared

mylocal_id = xslurm_shared.short_name(xslurm_shared.get_hostname())

WHITE=0
RED=1
GREEN=2
YELLOW=3
MAGENTA=4
BLUE=5
CYAN=6

TIMEOUT = 600

#current screen info
class gb:
    scr = None
    lock = None

    wlog = None
    wlog_currow = 0
    wlog_size = (0,0)

    wcom = None
    wcom_status = None
    
    wstatus = None
    wstatus_progress = None



class Servers:
    manager_server = None
    job_server = None


class Status(object):
    def __init__(self):
        self.stop_status_display = False
        self.address = ''
        self.port = 0
        self.kill_idle = False
        self.used_cpus = numpy.array([],dtype=float)
        self.used_mem = numpy.array([],dtype=float)
        self.total_cpus = numpy.array([],dtype=float)
        self.total_mem = numpy.array([],dtype=float)

    def set_address(self, address, port):
        self.address = address
        self.port = port

status = Status()


def parse_timeleft(time):
    time = time.split('-')
    if len(time) > 1:
        days = int(time[0])
        time = time[1]
    else:
        days = 0
        time = time[1]
    time = [int(e) for e in time.split(':')]
    
    if len(time) == 3:
        hours = time[0]
        time = time[1:]
    else:
        hours = 0

    if len(time) == 2:
        minutes = time[0]
        time = time[1:]
    else:
        minutes = 0
    seconds = time[0]
    
    return (days * 24 * 60 + hours * 60 + minutes)

queue_states = set(['PENDING', 'REQUEUED'])

class Job(object):
    def __init__(self, job_name, jobid, command, cwd, env, ncpu, mem, reqtime, requeue, dependency):
        self.job_name = job_name
        self.jobid = jobid
        self.command = command
        self.cwd = cwd
        self.env =env
        self.ncpu = ncpu
        self.mem = mem
        self.reqtime = reqtime
        self.requeue = requeue
        self.dependency = dependency

        self.state= 'PENDING'
        self.node_id = None
        self.starttime = None
        self.endtime = None




class JobManager(object):
    def __init__(self):
        self.jobid_counter = numpy.random.randint(0, 1000000)

        self.jobs_by_id = OrderedDict()

        self.jobs_done = []
        self.lock = threading.RLock()


        self.running_jobs = 0
        self.total_jobs = 0
    
    
    def get_job_stats(self):
        return (self.total_jobs, self.running_jobs)
    

    def active_engines(self):
        nodes = set()
        for job in self.jobs_by_id.values():
            if not job.node_id is None:
                nodes.add(job.node_id)
        return nodes
   


    def prioritize(self, job_pattern):
        self.lock.acquire()
        try:
            ndict = OrderedDict()

            for jobid, job in self.jobs_by_id.items():
                if job_pattern in job.job_name:
                    ndict[jobid] = job
            for jobid, job in self.jobs_by_id.items():
                if not job_pattern in job.job_name:
                    ndict[jobid] = job
            self.jobs_by_id = ndict
        finally:
            self.lock.release()

    def deprioritize(self, job_pattern):
        self.lock.acquire()
        try:
            ndict = OrderedDict()

            for jobid, job in self.jobs_by_id.items():
                if not job_pattern in job.job_name:
                    ndict[jobid] = job
            for jobid, job in self.jobs_by_id.items():
                if job_pattern in job.job_name:
                    ndict[jobid] = job
            self.jobs_by_id = ndict
        finally:
            self.lock.release()



    def submit_job(self, job_name, cmd, cwd, env, ncpu, mem, reqtime, requeue, dependency):
        self.lock.acquire()
        jobid = str(self.jobid_counter)
        self.jobid_counter += 1
        self.total_jobs += 1
        self.lock.release()
    

        #weird xml-rpc problem?
        if len(env) == 1 and 'data' in env:
            env = env['data']

        job = Job(job_name, jobid, cmd, cwd, env, ncpu, mem, reqtime, requeue, dependency)
        self.jobs_by_id[jobid] = job

        add_log_line(gb,"Job (id: %s, ncore: %d, mem: %d mb) submitted." % (jobid, ncpu, mem),GREEN)
        return jobid

    def cancel_job(self, jobid):
        self.lock.acquire()
        try:
            job = None
            if jobid in self.jobs_by_id:
                job = self.jobs_by_id[jobid]
                if job.state == 'RUNNING':
                    node_id = job.node_id
                    job.requeue = 0 #prevent restart
                    job.reason = "Awaiting cancellation at worker node"

                    try:
                        engines.send_command(node_id, xslurm_shared.CANCEL, jobid)
                    except:
                        pass

                    add_log_line(gb,"Job (id: %s, running on %s) has been send a cancel signal." % (job.jobid, job.node_id),YELLOW)
                    #now wait for cancel to propagate
                elif job.state == 'ASSIGNED':
                    node_id = job.node_id
                    try:
                        engines.send_command(node_id, xslurm_shared.DEASSIGN, jobid)
                    except:
                        pass
                    self.job_done(job.jobid, -1, 'Cancelled by user')
                else:
                    self.job_done(job.jobid, -1, 'Cancelled by user')
                    add_log_line(gb,"Job (id: %s) cancelled." % job.jobid,YELLOW)
                job.state = 'CANCELLED'
            else:
                add_log_line(gb,"Attempt to cancel unknown job (id: %s)." % jobid,YELLOW)
        finally:
            self.lock.release()

    def list_done_jobs(self):
        jobs = []
        for job in self.jobs_done:
            if not job.starttime is None and not job.endtime is None:
                runtime = job.endtime - job.starttime
            else:
                runtime = 0.0

            jobs.append([job.jobid, job.job_name, job.state, runtime, job.ncpu, job.reason])

        return jobs        
 

    def list_jobs(self):
        jobs = []
        for job in self.jobs_by_id.values():
            if not job.starttime is None:
                runtime = time.time() - job.starttime
            else:
                runtime = 0.0

            if job.node_id is None:
                node_id = '(Resources)'
            else:
                node_id = job.node_id

            jobs.append([job.jobid, job.job_name, job.state, runtime, job.ncpu, node_id])

        return jobs        
    
    def request_jobs(self, myid,current_cpu, current_mem):
        #add_log_line(gb,"Node %s requests for job with %d cpus and %d mb mem within %s time" % (myid, current_cpu, current_mem, (5 * 24 * 60.0) - current_time), BLUE)
        if not myid in engines.engine_by_id:
            return []

        timeleft = engines.engine_by_id[myid].timeleft

        self.lock.acquire()
        try:
            allres = []
            for job in self.jobs_by_id.values():
                #FIXME; assume 5 days of time
                if job.state in queue_states and job.reqtime <= timeleft:
                    if not (job.ncpu <= current_cpu and job.mem <= current_mem):
                        job.state = 'ASSIGNED'
                        add_log_line(gb,"Job (id: %s) assigned to node %s (%d/%d remaining)" % (job.jobid, myid, current_cpu-job.ncpu, current_mem-job.mem), BLUE)
                    else:
                        job.state = 'RUNNING'
                        add_log_line(gb,"Job (id: %s) started on node %s (%d/%d remaining)" % (job.jobid, myid, current_cpu-job.ncpu, current_mem-job.mem), BLUE)
                        self.running_jobs += 1
                        job.starttime = time.time()
               
                    current_cpu -= job.ncpu
                    current_mem -= job.mem

                    job.node_id = myid

                    res =  (job.jobid, job.command, job.cwd, job.env, job.ncpu, job.mem, job.state)
                    allres.append(res)
                    if current_cpu <= 0 or current_mem <= 0:
                        break
            
            if current_cpu > 0:
                for job in self.jobs_by_id.values():
                    #FIXME; assume 5 days of time
                    if job.state == 'ASSIGNED' and job.reqtime <= timeleft and job.ncpu <= current_cpu and job.mem <= current_mem:
                        try:
                            engines.send_command(job.node_id, xslurm_shared.DEASSIGN, job.jobid)
                        except KeyError:
                            pass

                        job.state = 'RUNNING'
                        add_log_line(gb,"Job (id: %s) reassigned and started on node %s (%d/%d remaining)" % (job.jobid, myid, current_cpu, current_mem), BLUE)
                        self.running_jobs += 1
                        job.starttime = time.time()
                
                        current_cpu -= job.ncpu
                        current_mem -= job.mem

                        job.node_id = myid

                        res =  (job.jobid, job.command, job.cwd, job.env, job.ncpu, job.mem, job.state)
                        allres.append(res)
                        if current_cpu <= 0 or current_mem <= 0:
                            break

        finally:            
            self.lock.release()
        return allres

    def engine_removed(self, engine_id, reason='Engine disappeared'):
        self.lock.acquire()
        try:
            for job in self.jobs_by_id.values():
                if job.node_id == engine_id:
                    if job.state == 'RUNNING':
                        self.job_done(job.jobid, -1, reason=reason)
                    elif job.state == 'ASSIGNED':
                        job.state = 'PENDING'
        finally:
            self.lock.release()

    def can_run_assigned_job(self, nodeid, jobid):
        self.lock.acquire()
        try:
            if not jobid in self.jobs_by_id:
                res = False
            else:
                job = self.jobs_by_id[jobid]
                res = (job.state == 'ASSIGNED')
                if res:                
                    self.job_start(nodeid, jobid)
        finally:
            self.lock.release()
        return res

    def job_start(self, node_id, jobid):
        self.lock.acquire()
        try:
            job = self.jobs_by_id[jobid]
            job.state = 'RUNNING'
            self.running_jobs += 1
            job.starttime = time.time()
            job.node_id = node_id
        finally:        
            self.lock.release()

    def job_done(self, jobid, return_code, reason='No Reason'):
        #FIXME: add in requeue stuff
        self.lock.acquire()
        try: 
            job = self.jobs_by_id.get(jobid,None)
            if not job is None:
                if not job.node_id is None:
                    job.endtime = time.time()
                    job.node_id = None
                    self.running_jobs -=1

                if return_code != 0 and job.requeue > 0:
                    job.requeue -= 1
                    job.state = 'REQUEUED'
                    job.starttime = None
                else:
                    job =  self.jobs_by_id[jobid]
                    del self.jobs_by_id[jobid]
                    self.jobs_done.append(job)
                    self.total_jobs -= 1

                job.return_code = return_code
                job.reason = reason
        finally:        
            self.lock.release()

    def job_finished(self, myid, jobid, return_code):
        self.lock.acquire()
        if return_code == 0:
            reason = 'Job Finished'
        elif return_code == 143 or return_code == 15:
            reason = 'Job Terminated'
        elif return_code == 137:
            reason = 'Job Killed'
        elif return_code == 130:
            reason = 'Job Interrupted'
        else:
            reason = 'Job Failed (errorcode: %d)' % return_code


        try:
            if not jobid in self.jobs_by_id:
                add_log_line(gb,"Unknown job (id: %s) completed at %s (%s)" % (jobid, myid, reason),YELLOW)
            else:
                self.job_done(jobid, return_code, reason)
        finally:
            self.lock.release()
        
        
        if return_code == 0:
            add_log_line(gb,"Job (id: %s) completed." % jobid,GREEN)
        else:
            add_log_line(gb,"Job (id: %s, node: %s) failed, reason: %s."  % (jobid, myid, reason),YELLOW)


jobs = JobManager()
submit_job = lambda *args: jobs.submit_job(*args)
cancel_job = lambda *args: jobs.cancel_job(*args)
list_jobs = lambda *args: jobs.list_jobs(*args)
list_done_jobs = lambda *args: jobs.list_done_jobs(*args)
request_jobs = lambda *args: jobs.request_jobs(*args)
job_finished = lambda *args: jobs.job_finished(*args)
can_run_assigned_job = lambda *args: jobs.can_run_assigned_job(*args)


def thread_start_job_server(port):
    server = SimpleXMLRPCServer(("", int(port)), logRequests = False, allow_none=True)
    server.register_function(submit_job, "submit_job")
    server.register_function(cancel_job, "cancel_job")
    server.register_function(list_jobs, "list_jobs")
    server.register_function(list_done_jobs, "list_done_jobs")
    Servers.job_server = server
    server.serve_forever()




class Engine(object):
    def __init__(self, engine_id, cores, totmem):
        self.engine_id = engine_id
        self.cluster_id = None

        self.cores = cores
        self.totmem = totmem
        self.starttime = time.time()
        self.timeleft = None
        self.lastseen = time.time()

        self.pending_commands = []
        self.jobs = []

        self.cpu_usage = 0.0
        self.mem_usage = 0.0



class EngineManager(object):
    def __init__(self):
        self.lock = threading.RLock()
        self.engine_by_id = {}
        
        self.engine_by_clusterid = {}
        self.local_engine_process = None

        self.cluster_queued_engines = 0
        self.cluster_running_engines = 0


    def send_command(self, nodeid, command, param=None):
        self.lock.acquire()
        try:
            engines.engine_by_id[nodeid].pending_commands.append((command,param))
        finally:    
            self.lock.release()

    

    def check_local_engine(self):
        if not self.local_engine_process is None and self.local_engine_process.poll() != None:
            self.lock.acquire()
            if not self.local_engine_process is None and self.local_engine_process.poll() != None: #recheck within lock
                self.local_engine_process = None
            self.lock.release()

        return self.local_engine_process != None
    
    
    def _check_cluster_engines(self):
        cmd = "squeue -h -r --user " + getpass.getuser() + " --name xslurm -o '%i|%L|%t|%N'" 

        args = shlex.split(cmd)
        p = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
        output = p.communicate()[0]

        output = [e.strip().split('|')  for e in output.split("\n") if e]

        z = defaultdict(int)
        
        observed_cluster_engines = set()
        observed_cluster_ids = set()
        nqueue = 0
        nrunning = 0
        for cid, ctime, cstate, cnode in output:
            observed_cluster_ids.add(cid)

            if cstate == 'R':
                nrunning += 1
                observed_cluster_engines.add(cnode)
            else:
                nqueue += 1
 
            if not cid in self.engine_by_clusterid:
                add_log_line(gb, 'Engine with SLURM id %s appeared suddenly...' % cid, YELLOW)
                self.engine_by_clusterid[cid] = None
            

            engine = self.engine_by_clusterid[cid]
            if cstate == 'R' and cnode in self.engine_by_id:
                engine = self.engine_by_id[cnode]
                engine.cluster_id = cid
                engine.timeleft = ctime
                self.engine_by_clusterid[cid] = engine                
            
            z[cstate] += 1

        for cid,engine in list(self.engine_by_clusterid.items()):
            if not engine is None:
                if not engine.engine_id in observed_cluster_engines:
                    add_log_line(gb, 'Engine %s disappeared from Slurm..' % engine.engine_id, YELLOW)
                    #should timeout automatically
                    del self.engine_by_clusterid[cid]
            elif not cid in observed_cluster_ids:
                #add_log_line(gb, 'Engine with SLURM id %s disappeared...' % cid, YELLOW)
                del self.engine_by_clusterid[cid]

        self.cluster_queued_engines = nqueue
        self.cluster_running_engines = nrunning


    def count_cluster_engines(self):
        return len(self.engine_by_clusterid)

    def start_local(self, gb, ncpu, mem):
        self.lock.acquire()
        try:
            if(not self.local_engine_process is None):
                set_status_message(gb,"Local engine already started!", YELLOW)
            elif(ncpu <= 0 or mem <= 0):
                set_status_message(gb,"Invalid cpu (%d) or mem (%.2g gb) constraint. " % (ncpu, mem / 1024.0), YELLOW)
            else: 
                set_status_message(gb,"Starting local engines", YELLOW)
                chief_path = "xslurm_chief"
                cmd = chief_path + " -a " + str(status.address) + " -p " + str(status.port) + " -c %d" % ncpu + " -m %d" % int(mem)
                add_log_line(gb,"Starting local engine using: " + cmd, GREEN)
                args = shlex.split(cmd)
            
                e = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
                log_output(e.stdout,gb,MAGENTA)
                log_output(e.stderr,gb,RED)
                self.local_engine_process = e

                set_status_message(gb,"Local engine started", GREEN)
                add_log_line(gb,"Local engine started", MAGENTA)
        finally:
            self.lock.release()

    def stop_local(self, gb):
        if(self.local_engine_process is None):
            set_status_message(gb,"No local engine running",YELLOW)
        else:
            set_status_message(gb,"Stopping local engine",YELLOW)
            stop_engine = self.local_engine_process
            if stop_engine.poll() is None:
                add_log_line(gb,"Termination signal send to local engine",MAGENTA)
                stop_engine.terminate()
            

            for engine in self.engine_by_id.values():
                if engine.cluster_id is False:
                    self.send_command(engine.engine_id, xslurm_shared.DIE) 
            
            counter = 65
            while counter > 0 and stop_engine.poll() is None:
                if (counter % 10) == 0:
                    add_log_line(gb,"Waiting %d seconds for local engine to terminate..." % counter,MAGENTA)
                time.sleep(1)
                counter -= 1
            if stop_engine.poll() is None:
                stop_engine.kill()
                add_log_line(gb,"Send local engine a kill signal...",RED)
            
            self.local_engine_process = None
            set_status_message(gb,"Local engine terminated",GREEN)

    #FIXME
    def start_slurm(self, gb,n, stime):
        self.lock.acquire()
        try:
            if(n < 0):
                set_status_message(gb,"Invalid number of engines to be started: " + str(n),YELLOW)
            else: 
                set_status_message(gb,"Starting SLURM engines",YELLOW)
                engine_path = "slurm_to_xslurm"
                if n > 1:
                    cmd = "sbatch -N 1 --parsable --array=0-%d -J xslurm -t " % (n-1)
                else:
                    cmd = "sbatch -N 1 --parsable -J xslurm -t "
                
                cmd = cmd + stime + " -p normal slurm_to_xslurm -a " + str(status.address) + " -p " + str(status.port)
                args = shlex.split(cmd)
            
                eid = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()[0]
                eid  = eid.strip()
                add_log_line(gb,"Queueing engines (%s) using: " % eid + cmd,GREEN)
                if 'error' in eid or not eid:
                    add_log_line(gb, 'Error during SLURM submission.', RED)
                    return

                if n > 1:
                    for i in range(n):
                        self.engine_by_clusterid[eid + '_' + str(i)] = None
                else:
                    self.engine_by_clusterid[eid] = None


                time.sleep(3)
                set_status_message(gb,"SLURM engines queued",GREEN)
                add_log_line(gb,str(n) + " SLURM engines queued",MAGENTA)
        finally:
            self.lock.release()

    #FIXME: jobs running removal, priority of cancelling less active nodes
    def stop_slurm(self, gb,n):
        self.lock.acquire()
        try:
            if(self.count_cluster_engines() == 0 and n != 0):
                set_status_message(gb,"No SLURM engines running",YELLOW)
            elif(n < 0 or n > self.count_cluster_engines()):
                set_status_message(gb,"Invalid number of engines to be stopped: " + str(n),YELLOW)
            else:
                set_status_message(gb,"Stopping SLURM engines",YELLOW)

                #stop queued engines
                stop_engines = []
                
                for cid, engine in self.engine_by_clusterid.items():
                    if engine is None:
                        stop_engines.append(cid)
               
                stop_engines.sort()
                stop_engines = stop_engines[::-1] #cancel youngest non-running engine

                #stop idle engines
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in self.engine_by_clusterid.items():
                        if engine is None:
                            continue
                        if len(engine.jobs) == 0: 
                            stop_engines2.append(cid)
                    stop_engines2.sort() #cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2

                #stop oldest engines
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in self.engine_by_clusterid.items():
                        if engine is None:
                            continue
                        if not cid in stop_engines:
                            stop_engines2.append(cid)
                    stop_engines2.sort() #cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2


                stop_engines = stop_engines[:n]
                
                add_log_line(gb, "Stop engines: %s" % str(stop_engines), CYAN)
                
                set_progress_bar(gb,0.0)
                for pos, eid in enumerate(stop_engines):
                    self.cancel_cid(eid)
                    set_progress_bar(gb,float(pos+1)/float(n))
                
                time.sleep(2)
                stop_progress_bar(gb)
                set_status_message(gb,"SLURM engines stopped",GREEN)
                add_log_line(gb,str(n) + " SLURM engines stopped",MAGENTA)
        finally: 
            self.lock.release()

    def cancel_cid(self, cid):
        self.lock.acquire()
        try:
            cmd = "scancel " + str(cid)
            args = shlex.split(cmd)
            add_log_line(gb, "Cancelling using command: " + cmd, CYAN)
            p = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
            log_output(p.stdout,gb,WHITE)
            log_output(p.stderr,gb,RED)
            p.wait()
            
            if cid in self.engine_by_clusterid:
                e = self.engine_by_clusterid[cid]
                if not e is None:
                    self._unregister(e.engine_id, reason='Engine stopped')
        finally:
            self.lock.release()
   

    def stop_all(self):
        if self.count_cluster_engines() > 0:
            add_log_line(gb,"Terminating SLURM engines.", RED)
            self.stop_slurm(gb, self.count_cluster_engines())
        if self.check_local_engine():
            add_log_line(gb,"Terminating local engine.", RED)
            self.stop_local(gb)
    
    
    def register(self, myip,cores,totmem):
        self.lock.acquire()
        try:
            myid = xslurm_shared.short_name(myip)
            if myid in self.engine_by_id:
                myid = 'DENIED'
                add_log_line(gb,"Secondary engine on " + myid + ' was denied',RED)
            else:
                e = Engine(myid, cores, totmem)
                self.engine_by_id[myid] = e
                if myid == status.address.split('-')[0]:
                    e.cluster_id = False
                    e.timeleft = 5 * 24 * 60
            
                totalgb = totmem / 1024.0
                average_use = totalgb / float(cores)
            
                add_log_line(gb,"Engine (" + str(cores) + "C:" + "{:1.1f}".format(average_use) + " GB) on " + myid + ' is now online',CYAN)
        
        except Exception, e:            
            add_log_line(gb,"Exception in register: " + str(e),RED)
        finally:
            self.lock.release()
        return myid

    def poll(self, myid, cpu_usage, mem_usage, mode, idle):
        try:
            e = self.engine_by_id[myid]
            e.cpu_usage = cpu_usage
            e.mem_usage = mem_usage
            e.lastseen = time.time()
           
            commands = e.pending_commands
            # kill if idle, but not if local engine or if started in last 60 seconds
            if idle and status.kill_idle and not myid == status.address.split('-')[0] and not (e.lastseen - e.starttime) < 60):
                add_log_line(gb,"Stopping idle engine: " + myid, YELLOW)

                commands.append((xslurm_shared.STOP,None))

            if commands:
                self.lock.acquire()
                commands = e.pending_commands
                e.pending_commands = []
                self.lock.release()


        except KeyError:
            commands = []
            if mode == xslurm_shared.STOPPING:
                add_log_line(gb,"Poll for unknown engine id: " + myid + ". Engine is stopping.",RED)
                commands.append((xslurm_shared.STOP,None))
            else:
                add_log_line(gb,"Poll for unknown engine id: " + myid + ". Taking ownership.",RED)
                commands.append((xslurm_shared.REREGISTER,None))

        return commands
            
    def unregister(self, myid):
        self._unregister(myid, 'Engine unregistered')
        add_log_line(gb,"Engine with id " + myid + " unregistered",CYAN)
        return True

    def terminate_engine(self, myid):
        self.lock.acquire()
        try:
            e = self.engine_by_id[myid]
            if not (e.cluster_id is None or e.cluster_id is False) and e.cluster_id in self.engine_by_clusterid:
                self.cancel_cid(e.cluster_id)
            if e.cluster_id is False:
                self.stop_local(gb)
        finally:
            self.lock.release()

    def _unregister(self, myid, reason='Engine disappeared'):
        jobs.engine_removed(myid, reason=reason)
        self.lock.acquire()
        try:
            if myid in self.engine_by_id:
                e = self.engine_by_id[myid]
                if e.cluster_id in self.engine_by_clusterid:
                    self.engine_by_clusterid[e.cluster_id] = None 
                del self.engine_by_id[myid]
            
        finally:
            self.lock.release()
    
    

engines = EngineManager()
unregister = lambda *args: engines.unregister(*args)
poll = lambda *args: engines.poll(*args)
register = lambda *args: engines.register(*args)

def thread_check_commands():
    while 1:
        
        engines.lock.acquire()
        try:
            engines._check_cluster_engines()
            all_engines = engines.engine_by_id.values()
        finally:
            engines.lock.release()
        
        now = time.time()
        for engine in all_engines:
            if now - engine.lastseen > TIMEOUT:
                engines.terminate_engine(engine.engine_id)
                engines._unregister(engine.engine_id, 'Engine timed out')
                add_log_line(gb,"Engine with id " + engine.engine_id + " timed out",RED)
        
        used_cpus = []
        used_mem = []
        total_cpus = []
        total_mem = []
        for engine in all_engines:
            total_cpus.append(engine.cores)
            total_mem.append(engine.totmem / 1024.0)

            used_cpus.append(engine.cores * (engine.cpu_usage / 100.0))
            used_mem.append((engine.totmem / 1024.0) * (engine.mem_usage/100.0))


        status.used_cpus = numpy.array(used_cpus)
        status.total_cpus = numpy.array(total_cpus,dtype=float)
        status.used_mem = numpy.array(used_mem,dtype=float)
        status.total_mem = numpy.array(total_mem,dtype=float)

        time.sleep(5)



def thread_start_manager_server(port):
    server = SimpleXMLRPCServer(("", int(port)), logRequests = False, allow_none=True)
    server.register_function(register, "register")
    server.register_function(unregister, "unregister")
    server.register_function(poll, "poll")
    server.register_function(request_jobs, "request_jobs")
    server.register_function(job_finished, "job_finished")
    server.register_function(can_run_assigned_job, 'can_run_assigned_job')

    Servers.manager_server = server
    server.serve_forever()


def start_server(port):
    t = threading.Thread(target=thread_start_manager_server,args=(port,))
    t.daemon=True
    t.start()
    
    t = threading.Thread(target=thread_start_job_server,args=(port+1,))
    t.daemon=True
    t.start()

    t = threading.Thread(target=thread_check_commands)
    t.daemon=True
    t.start()


#create a bordered window
def border_win(parent,height,width,x,y,**kwargs):
    if(not 'notop' in kwargs):
        parent.hline(x,y,'.',width)
        height -= 1
        x += 1
    if(not 'nobottom' in kwargs):
        parent.hline(x+height-1,y,'.',width)
        height -= 1
    if(not 'noleft' in kwargs):
        parent.vline(x,y,'.',height)
        width -= 1
        y += 1
    if(not 'noright' in kwargs):
        parent.vline(x,y+width-1,'.',height)
        width -= 1
    w = parent.subwin(height, width,x,y)
    w.noutrefresh()
    parent.refresh()
    return w


#threaded output of a file to the log window
def thread_log_output(file, gb, color, filter):
    while 1 :
        line = file.readline()
        if (not line):
            break
         
        if (filter and len([f for f in filter if f in line]) > 0) :
            pass
        else:    
            add_log_line(gb,line,color)

def log_output(file, gb, color = 0, filter = []):
    t = thread.start_new_thread(thread_log_output, (file, gb, color, filter))
    return t



def get_stat_str(stats):
    cpu,memp,mempmax,mem,memmax = stats
    if(cpu < 0.50 or mempmax > 0.90):
        color = RED
    elif(cpu < 0.80 or mempmax > 0.70):
        color = YELLOW
    else:
        color = GREEN

    descr = "(" +  '{:.2%}'.format(cpu) + " CPU, " + \
            '{:.2%}'.format(memp) + "/" + '{:.2%}'.format(mempmax)  + "/" + \
            '{}'.format(int(mem)) + "MB MEM)"
    return descr,color

#thread display status
def thread_display_status(gb,status):
    while not status.stop_status_display:
        local_running = engines.check_local_engine()
        equeue = engines.cluster_queued_engines
        erunning = engines.cluster_running_engines
        njobs, nrunning = jobs.get_job_stats()

        #active_engines = jobs.active_engines()
        #active_engines.discard(mylocal_id)
        #eactive = len(active_engines)
        
        gb.lock.acquire()
        try:
            if(gb.wcom_status and gb.wcom_status.getmaxyx()[1] > 50):
                sizey = gb.wcom_status.getmaxyx()[1]
                gb.wcom_status.erase()
                gb.wcom_status.addstr(1,1,"Jobs: ")
                gb.wcom_status.addstr(1,18,"%d/%d" % (nrunning, njobs),curses.color_pair(GREEN))
                
                
                gb.wcom_status.addstr(3,1,"Local engine: ")
                if not local_running:
                   gb.wcom_status.addstr(3,18,"NOT RUNNING",curses.color_pair(YELLOW))
                else:
                   gb.wcom_status.addstr(3,18,"RUNNING",curses.color_pair(GREEN))

                gb.wcom_status.addstr(5,1,"SLURM engines: ")
                enodes = erunning + equeue

                cpu_c = GREEN
                mem_c = GREEN
                if len(status.used_cpus) > 0:
                    cpu_usage = numpy.percentile(status.used_cpus / status.total_cpus,[0,25,50,75,100])
                    mem_usage = numpy.percentile(status.used_mem / status.total_mem,[0,25,50,75,100])
                    
                    if cpu_usage[1] < 0.5 or cpu_usage[2] < 0.75:
                        cpu_c = YELLOW
                    if cpu_usage[1] < 0.1 or cpu_usage[2] < 0.2:
                        cpu_c = RED

                    if mem_usage[-1] > 0.9 or mem_usage[-2] > 0.8 or mem_usage[-3] > 0.7:
                        mem_c = YELLOW
                    if mem_usage[-1] > 0.99 or mem_usage[-2] > 0.95 or mem_usage[-3] > 0.9:
                        mem_c = RED
                else:
                    cpu_usage = numpy.zeros(5,dtype=float)
                    mem_usage = numpy.zeros(5,dtype=float)

                gb.wcom_status.addstr(7,1,"CPU usage: ")
                gb.wcom_status.addstr(7,18, "%d/%d (%.2f - %.2f - %.2f - %.2f - %.2f)" \
                        % ((numpy.sum(status.used_cpus), numpy.sum(status.total_cpus)) + tuple(cpu_usage)), curses.color_pair(cpu_c))

                gb.wcom_status.addstr(9,1,"Mem usage (GB): ")
                gb.wcom_status.addstr(9,18, "%d/%d (%.2f - %.2f - %.2f - %.2f - %.2f)" \
                        % ((numpy.sum(status.used_mem), numpy.sum(status.total_mem)) + tuple(mem_usage)), curses.color_pair(mem_c))
                                                                                                           
                
                if equeue == 0:
                    color = GREEN
                else:
                    color = YELLOW
                gb.wcom_status.addstr(5,18,"%d/%d" % (erunning,enodes),curses.color_pair(color))

                
                gb.wcom_status.refresh()
        except Exception,e:
            add_log_line(gb,"Exception in display thread: " + str(e), RED)
        finally:
            gb.lock.release()
        time.sleep(1)

def start_status_display(gb,status):
    status.stop_status_display = False
    t = threading.Thread(target=thread_display_status,args=(gb,status))
    t.daemon=True
    t.start()
    return t

def stop_status_display(status,t):
    set_status_message(gb, "Waiting for status thread to terminate..", YELLOW)
    status.stop_status_display = True
    t.join()

#initialize windows/lock, etc.
def build_windows(gb):
    if(not gb.lock): 
        gb.lock = threading.RLock()
    gb.lock.acquire()
    try:
        
        #clear screen in curses does not work somehow. 
        print "\033[2J"

        gb.scr.erase()
        gb.scr.refresh()
        curses.start_color()
        # Invisibility mode of the cursor may not be supported by the terminal
        try :
            curses.curs_set(0)
        except :
            pass
        
        #curses getmaxyx does not work for resize screens to smaller sizes. Use stty instead. 
        rows, columns = os.popen('stty size', 'r').read().split()       
        size = (int(rows), int(columns))

        com_win_rows = max(0,min(20,size[0]-2))
        status_win_rows = max(0,min(2,size[0] - com_win_rows))
        log_win_rows = max(0,size[0] - com_win_rows - status_win_rows)


        #init colors
        curses.init_pair(RED,curses.COLOR_RED,curses.COLOR_BLACK)
        curses.init_pair(GREEN,curses.COLOR_GREEN,curses.COLOR_BLACK)
        curses.init_pair(YELLOW,curses.COLOR_YELLOW,curses.COLOR_BLACK)
        curses.init_pair(MAGENTA,curses.COLOR_MAGENTA,curses.COLOR_BLACK)
        curses.init_pair(BLUE,curses.COLOR_BLUE,curses.COLOR_BLACK)
        curses.init_pair(CYAN,curses.COLOR_CYAN,curses.COLOR_BLACK)
        
        #log window
        if(log_win_rows > 2):
            gb.wlog = border_win(gb.scr,log_win_rows,size[1],0,0)
            gb.wlog_size = gb.wlog.getmaxyx()
            gb.wlog_currow = 0
            gb.wlog.setscrreg(0,gb.wlog_size[0]-1)
            gb.wlog.idlok(True)
            gb.wlog.scrollok(True)
            gb.wlog.refresh()
        else:
            gb.wlog = None

        #com window
        if(com_win_rows > 10):
            com_win_cols = size[1] / 2
            if(com_win_cols > 55):
                comlog_win_cols = size[1] - 55
                com_win_cols = 55
                gb.wcom_status = border_win(gb.scr,com_win_rows,comlog_win_cols,log_win_rows,com_win_cols,notop=True,noleft=True)
                gb.wcom_status.refresh()
            else:
                com_win_cols = size[1]
                gb.wcom_status = None
        
            gb.wcom = border_win(gb.scr,com_win_rows,com_win_cols,log_win_rows,0,notop=True)
            gb.wcom.refresh()
        else:
            gb.wcom = None
            gb.wcom_status = None
        
        #status window
        if(status_win_rows == 2):
            gb.wstatus = gb.scr.subwin(status_win_rows,size[1],log_win_rows + com_win_rows,0)
            gb.wstatus.refresh()
        else:
            gb.wstatus = None

        init_commands(gb)
        gb.scr.refresh()
    except:
        pass
    finally:
        gb.lock.release()    

#add line to log windows
def add_log_line(gb,line,color=0):
    gb.lock.acquire()
    try:
        if(line and line[-1] == '\n'):
            gb.log_file.write(line)
        else:
            gb.log_file.write(line + '\n')
        if(gb.wlog):
            try:
                line = str(line.decode('ascii'))
            except UnicodeDecodeError:
                line = "Invalid line format (unicode) encountered"
                color = RED
            line = line[:(gb.wlog_size[1] - 2)]
            if(line and line[-1] == '\n'): line = line[:-1]
            if(gb.wlog_currow < gb.wlog_size[0]):
                gb.wlog.addstr(gb.wlog_currow,1,line,curses.color_pair(color))
                gb.wlog_currow += 1
            else:
                gb.wlog.scroll()
                gb.wlog.addstr(gb.wlog_size[0] - 1,1,line,curses.color_pair(color))
            gb.wlog.refresh()
    except Exception, e:
        print e
    finally:
        gb.lock.release()

#set status message iin status window
def set_status_message(gb, line, color = 0):
    gb.lock.acquire()
    try:
        if (gb.wstatus) :
            gb.wstatus.addstr(0,0,line,curses.color_pair(color))
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

def set_progress_bar(gb,s):
    gb.lock.acquire()
    try:
        if(gb.wstatus):
            if(s < 0.0 or s > 1.0):
                add_log_line(gb, "Progress bar value out of range: " + str(s),RED)
            else:
                size = gb.wstatus.getmaxyx()
                total_i = size[1] - 3
                full_i = int(s * total_i)
                rem_i = total_i - full_i
                line = "[" + ('#' * full_i) + (' ' * rem_i) + "]"
                gb.wstatus.addstr(1,0,line,curses.color_pair(MAGENTA))
                gb.wstatus.clrtoeol()
                gb.wstatus.refresh()
    except Exception,e:
        add_log_line(gb,"Exception in set_progress_bar: " + str(e),RED)
    finally:
        gb.lock.release()

def stop_progress_bar(gb):
    gb.lock.acquire()
    try:
        if(gb.wstatus):
            gb.wstatus.addstr(1,0,"")
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

#print commands in command windows
def init_commands(gb):
    gb.lock.acquire()
    try:
        if not gb.wcom is None:
            gb.wcom.addstr(1,1,"Start/stop local engine: s/x")
            gb.wcom.addstr(2,1,"Start/stop SLURM engines: d/c [n]")
            gb.wcom.addstr(4,1,"Toggle autostop idle engines: k")
            gb.wcom.addstr(5,1,"Prioritize jobs: p")
            gb.wcom.addstr(6,1,"Deprioritize jobs: n")
            #gb.wcom.addstr(5,1,"Autogrow SLURM engines: g")
            #gb.wcom.addstr(6,1,"Autocancel SLURM engines: h")
            gb.wcom.addstr(9,1,"Shutdown: q")
            gb.wcom.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

#enter command in command window
def enter_command(gb,argument="Command: "):
    if gb.wcom is None:
        return
    gb.lock.acquire()
    try:
        command = ""
        size = gb.wcom.getmaxyx()
        x = size[0] - 1
        gb.wcom.addstr(x,1,argument)
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
        while 1:
            gb.lock.release()
            c = gb.scr.getch()
            gb.lock.acquire()
            if c == curses.ascii.LF:
                break
            elif c == curses.ascii.DEL:
                command = command[:-1]
            else:
                if(c > 256):
                    set_status_message(gb,"Keycode not supported: " + str(c))
                else:
                    command += chr(c)
            gb.wcom.addstr(x,1,argument + command[:(size[1] - len(argument) - 2)])
            gb.wcom.clrtoeol()
            gb.wcom.refresh()

        gb.wcom.addstr(x,1,"")
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
    finally:
        gb.lock.release()
    return command

def get_number(gb,argument,default=0):
    cmd = enter_command(gb,argument)
    if(not cmd):
        n = default
    else:
        try:
            n = int(cmd)
        except ValueError:
            set_status_message(gb,"Number of engines should be a number",YELLOW)
            n = None
    return n







def main(scr, *args, **kwds):
    address = xslurm_shared.get_hostname()
    port = xslurm_shared.port

    status.set_address(address,port)
    
    gb.scr = scr
    gb.log_file = open('cluster.log','w')

    build_windows(gb)
    t = start_status_display(gb,status)
    start_server(port)
   

    #workaround for bug in curses: no resize messages
    os.environ['LINES']="blah"
    del os.environ['LINES']
    os.environ['COLUMNS']="blah"
    del os.environ['COLUMNS']

    try:
        while 1:
            c = gb.scr.getch()
            if c == curses.KEY_RESIZE:
                build_windows(gb)
            elif c == ord('q'):  #QUIT
                if engines.check_local_engine() or engines.count_cluster_engines() > 0:
                    ans = enter_command(gb, "Are you SURE? Engines are STOPPED! (yes/no): ")
                else:
                    ans = enter_command(gb, "Are you sure? (yes/no): ")
                if ans == 'yes':
                    if engines.check_local_engine() or engines.count_cluster_engines() > 0:
                        engines.stop_all()
                    stop_status_display(status,t)
                    gb.scr.refresh()
                    break
            elif c == ord('s'):  #START local engine
                cpus = get_number(gb, "Number of cpus (def=23): ", 23)
                mem = get_number(gb, "Maximal memory in GB (def=62): ", 62)
                if(not cpus is None and not mem is None):
                    engines.start_local(gb,cpus,mem * 1024.0)

            elif c == ord('x'):  #STOP local engines
                engines.stop_local(gb)

            elif c == ord('d'):  #START SLURM engines
                cmd = get_number(gb, "Number of engines (def=1): ",1)
                runtime = enter_command(gb, "Enter time (default=5-0:0:0): ")
                if runtime == "":
                    runtime = '5-0:0:0'
                if(not cmd is None):
                    engines.start_slurm(gb,cmd, runtime)
            elif c == ord('p'):
                jobpattern = enter_command(gb, 'Enter job name pattern to prioritize: ')
                if not jobpattern == '':
                    jobs.prioritize(jobpattern)
            elif c == ord('n'):
                jobpattern = enter_command(gb, 'Enter job name pattern to deprioritize: ')
                if not jobpattern == '':
                    jobs.deprioritize(jobpattern)
            elif c == ord('c'):  #STOP PBS engines
                cmd = get_number(gb, "Number of engines (def=all): ",engines.count_cluster_engines())
                if(not cmd is None):
                    engines.stop_slurm(gb,cmd)

            elif c == ord('k'): #KILL IDLE toggle
                status.kill_idle = not status.kill_idle
                add_log_line(gb, "Autostop idle engine set to " + ("ON" if status.kill_idle else 'OFF') ,GREEN)
    	    elif c == ord('u') :  # Update ncurses screen
                gb.scr.refresh()
    except Exception:
        add_log_line(gb,"EXCEPTION!!! EMERGENCY SHUTDOWN IN PROGRESS!",RED)
        try:
            engines.stop_all()
            stop_status_display(status,t)
        except Exception:
            pass
        raise
        
    time.sleep(2) #wait for all paint ops to finish and engines to terminate
curses.wrapper(main)
