#!/usr/bin/env python
import curses, curses.textpad, curses.ascii
import os
import subprocess, sys, getopt, time,shlex
from subprocess import Popen
import thread
import threading
from collections import defaultdict, OrderedDict

from SimpleXMLRPCServer import SimpleXMLRPCServer
import random
import psutil
import numpy
import getpass

import xslurm_shared


WHITE=0
RED=1
GREEN=2
YELLOW=3
MAGENTA=4
BLUE=5
CYAN=6

TIMEOUT = 600

#current screen info
class gb:
    scr = None
    lock = None

    wlog = None
    wlog_currow = 0
    wlog_size = (0,0)

    wcom = None
    wcom_status = None
    
    wstatus = None
    wstatus_progress = None



class Servers:
    manager_server = None
    job_server = None


class Status(object):
    def __init__(self):
        self.stop_status_display = False
        self.address = ''
        self.port = 0

    def set_address(self, address, port):
        self.address = address
        self.port = port

status = Status()



queue_states = set(['PENDING', 'REQUEUED'])

class Job(object):
    def __init__(self, job_name, jobid, command, cwd, env, ncpu, mem, reqtime, requeue, dependency):
        self.job_name = job_name
        self.jobid = jobid
        self.command = command
        self.cwd = cwd
        self.env =env
        self.ncpu = ncpu
        self.mem = mem
        self.reqtime = reqtime
        self.requeue = requeue
        self.dependency = dependency

        self.state= 'PENDING'
        self.node_id = None
        self.starttime = None
        self.endtime = None




class JobManager(object):
    def __init__(self):
        self.jobid_counter = numpy.random.randint(0, 1000000)

        self.jobs_by_id = OrderedDict()

        self.jobs_done = []
        self.lock = threading.RLock()


        self.running_jobs = 0
        self.total_jobs = 0
    
    
    def get_job_stats(self):
        return (self.total_jobs, self.running_jobs)
    

    def submit_job(self, job_name, cmd, cwd, env, ncpu, mem, reqtime, requeue, dependency):
        self.lock.acquire()
        jobid = str(self.jobid_counter)
        self.jobid_counter += 1
        self.total_jobs += 1
        self.lock.release()
    

        #weird xml-rpc problem?
        if len(env) == 1 and 'data' in env:
            env = env['data']

        job = Job(job_name, jobid, cmd, cwd, env, ncpu, mem, reqtime, requeue, dependency)
        self.jobs_by_id[jobid] = job

        add_log_line(gb,"Job (id: %s, ncore: %d, mem: %d mb) submitted." % (jobid, ncpu, mem),GREEN)
        return jobid

    def cancel_job(self, jobid):
        self.lock.acquire()
        try:
            job = None
            if jobid in self.jobs_by_id:
                job = self.jobs_by_id[jobid]
                if job.state == 'RUNNING':
                    node_id = job.node_id
                    job.requeue = 0 #prevent restart
                    job.reason = "Awaiting cancellation at worker node"

                    engines.lock.acquire()
                    engines.engine_by_id[node_id].pending_commands.append((CANCEL,jobid))
                    engines.lock.release()

                    add_log_line(gb,"Job (id: %s, running on %s) has been send a cancel signal." % (job.jobid, job.node_id),YELLOW)
                    #now wait for cancel to propagate
                else:
                    job_done(job.jobid, -1, 'Cancelled by user')
                    add_log_line(gb,"Job (id: %s) cancelled." % job.jobid,YELLOW)
                job.state = 'CANCELLED'
            else:
                add_log_line(gb,"Attempt to cancel unknown job (id: %s)." % jobid,YELLOW)
        finally:
            self.lock.release()

    def list_done_jobs(self):
        jobs = []
        for job in self.jobs_done:
            if not job.starttime is None and not job.endtime is None:
                runtime = job.endtime - job.starttime
            else:
                runtime = 0.0

            jobs.append([job.jobid, job.job_name, job.state, runtime, job.ncpu, job.reason])

        return jobs        
 

    def list_jobs(self):
        jobs = []
        for job in self.jobs_by_id.values():
            if not job.starttime is None:
                runtime = time.time() - job.starttime
            else:
                runtime = 0.0

            if job.node_id is None:
                node_id = '(Resources)'
            else:
                node_id = job.node_id

            jobs.append([job.jobid, job.job_name, job.state, runtime, job.ncpu, node_id])

        return jobs        
    
    def request_job(self, myid,current_cpu, current_mem, current_time):
        res =  (None, None, None, None, None, None)
        
        #add_log_line(gb,"Node %s requests for job with %d cpus and %d mb mem within %s time" % (myid, current_cpu, current_mem, (5 * 24 * 60.0) - current_time), BLUE)

        if not myid in engines.engine_by_id:
            return res

        self.lock.acquire()
        try:
            for job in self.jobs_by_id.values():
                #FIXME; assume 5 days of time
                if job.state in queue_states and job.ncpu <= current_cpu and job.mem <= current_mem and job.reqtime <= (5 * 24 * 60.0) - current_time:
                    add_log_line(gb,"Job (id: %s) started on node %s (%d/%d remaining)" % (job.jobid, myid, current_cpu, current_mem), BLUE)
                    
                    job.state = 'RUNNING'
                    job.node_id = myid
                    job.starttime = time.time()

                    res =  (job.jobid, job.command, job.cwd, job.env, job.ncpu, job.mem)
                    self.running_jobs += 1
                    break
        finally:            
            self.lock.release()
        return res


    def job_done(self, jobid, return_code, reason='No Reason'):
        #FIXME: add in requeue stuff
        self.lock.acquire()
        try: 
            job = self.jobs_by_id.get(jobid,None)
            if not job is None:
                if not job.node_id is None:
                    job.endtime = time.time()
                    job.node_id = None
                    self.running_jobs -=1

                if return_code != 0 and job.requeue > 0:
                    job.requeue -= 1
                    job.state = 'REQUEUED'
                    job.starttime = None
                else:
                    job =  self.jobs_by_id[jobid]
                    del self.jobs_by_id[jobid]
                    self.jobs_done.append(job)
                    self.total_jobs -= 1

                job.return_code = return_code
                job.reason = reason
        finally:        
            self.lock.release()

    def job_finished(self, myid, jobid, return_code):
        self.lock.acquire()
        if return_code == 0:
            reason = 'Job Finished'
        elif return_code == 143:
            reason = 'Job Terminated'
        elif return_code == 137:
            reason = 'Job Killed'
        elif return_code == 130:
            reason = 'Job Interrupted'
        else:
            reaseon = 'Job Failed (errorcode: %d)' % return_code


        try:
            if not jobid in self.jobs_by_id:
                add_log_line(gb,"Unknown job (id: %s) completed at %s (%s)" % (jobid, myid, reason),YELLOW)
            else:
                self.job_done(jobid, return_code, reason)
        finally:
            self.lock.release()
        
        
        if return_code == 0:
            add_log_line(gb,"Job (id: %s) completed." % jobid,GREEN)
        else:
            add_log_line(gb,"Job (id: %s, node: %s) failed, reason: %s."  % (jobid, myid, reason),YELLOW)


jobs = JobManager()
submit_job = lambda *args: jobs.submit_job(*args)
cancel_job = lambda *args: jobs.cancel_job(*args)
list_jobs = lambda *args: jobs.list_jobs(*args)
list_done_jobs = lambda *args: jobs.list_done_jobs(*args)
request_job = lambda *args: jobs.request_job(*args)
job_finished = lambda *args: jobs.job_finished(*args)


def thread_start_job_server(port):
    server = SimpleXMLRPCServer(("", int(port)), logRequests = False, allow_none=True)
    server.register_function(submit_job, "submit_job")
    server.register_function(cancel_job, "cancel_job")
    server.register_function(list_jobs, "list_jobs")
    server.register_function(list_done_jobs, "list_done_jobs")
    Servers.job_server = server
    server.serve_forever()




class Engine(object):
    def __init__(self, engine_id, cores, totmem):
        self.engine_id = engine_id
        self.cluster_id = None

        self.cores = cores
        self.totmem = totmem
        self.starttime = time.time()
        self.timeleft = None
        self.lastseen = time.time()

        self.pending_commands = []
        self.jobs = []

        self.cpu_usage = 0.0
        self.mem_usage = 0.0



class EngineManager(object):
    def __init__(self):
        self.lock = threading.RLock()
        self.engine_by_id = {}
        
        self.engine_by_clusterid = {}
        self.local_engine_process = None

        self.cluster_queued_engines = 0
        self.cluster_running_engines = 0
        

    def check_local_engine(self):
        if not self.local_engine_process is None and self.local_engine_process.poll() != None:
            self.lock.acquire()
            if not self.local_engine_process is None and self.local_engine_process.poll() != None: #recheck within lock
                self.local_engine_process = None
            self.lock.release()

        return self.local_engine_process != None
    
    
    def _check_cluster_engines(self):
        cmd = "squeue -h --user " + getpass.getuser() + " --name xslurm -o '%A|%L|%t|%B'" 

        args = shlex.split(cmd)
        p = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
        output = p.communicate()[0]

        output = [e.strip().split('|')  for e in output.split("\t") if e]

        z = defaultdict(int)
        
        observed_cluster_engines = set()
        for cid, ctime, cstate, cnode in output:
            if not cid in self.engine_by_clusterid:
                continue
                #add_log_line(gb, 'Engine with SLURM id %s appeared suddenly...' % cid, YELLOW)
                #self.engine_by_clusterid[cid] = None
            observed_cluster_engines.add(cid)

            engine = self.engine_by_clusterid[cid]
            if not engine is None:
                engine.timeleft = ctime
            elif cstate == 'R' and cnode in self.engine_by_id:
                engine = self.engine_by_id[cnode]
                engine.cluster_id = cid
                engine.timeleft = ctime
                self.engine_by_clusterid[cid] = engine

            z[cstate] += 1

        for cid,engine in self.engine_by_clusterid.items():
            if not cid in observed_cluster_engines:
                if not engine is None:
                    add_log_line(gb, 'Engine %s disappeared from Slurm..' % engine.engine_id, YELLOW)
                    pass #should timeout automatically....
                else:
                    add_log_line(gb, 'Engine with SLURM id %s disappeared...' % cid, YELLOW)
                del self.engine_by_clusterid[cid]

        self.cluster_queued_engines = z['PD'] + z['CF']
        self.cluster_running_engines = z['R']


    def get_cluster_engines(self):
        return self.engine_by_clusterid.keys()

    def start_local(self, gb, ncpu, mem):
        self.lock.acquire()
        try:
            if(not self.local_engine_process is None):
                set_status_message(gb,"Local engine already started!", YELLOW)
            elif(ncpu <= 0 or mem <= 0):
                set_status_message(gb,"Invalid cpu (%d) or mem (%.2g gb) constraint. " % (ncpu, mem / 1024.0), YELLOW)
            else: 
                set_status_message(gb,"Starting local engines", YELLOW)
                chief_path = "xslurm_chief"
                cmd = chief_path + " -a " + str(status.address) + " -p " + str(status.port) + " -c %d" % ncpu + " -m %d" % int(mem)
                add_log_line(gb,"Starting local engine using: " + cmd, GREEN)
                args = shlex.split(cmd)
            
                e = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
                log_output(e.stdout,gb,MAGENTA)
                log_output(e.stderr,gb,RED)
                self.local_engine_process = e

                set_status_message(gb,"Local engine started", GREEN)
                add_log_line(gb,"Local engine started", MAGENTA)
        finally:
            self.lock.release()

    def stop_local(self, gb):
        if(self.local_engine_process is None):
            set_status_message(gb,"No local engine running",YELLOW)
        else:
            set_status_message(gb,"Stopping local engine",YELLOW)
            stop_engine = self.local_engine_process
            if stop_engine.poll() is None:
                add_log_line(gb,"Termination signal send to local engine",MAGENTA)
                stop_engine.terminate()
            

            for engine in self.engine_by_id.values():
                if engine.cluster_id is False:
                    engine.pending_commands.append((xslurm_shared.DIE,None)) 
            
            counter = 65
            while counter > 0 and stop_engine.poll() is None:
                if (counter % 10) == 0:
                    add_log_line(gb,"Waiting %d seconds for local engine to terminate..." % counter,MAGENTA)
                time.sleep(1)
                counter -= 1
            if stop_engine.poll() is None:
                stop_engine.kill()
                add_log_line(gb,"Send local engine a kill signal...",RED)
            
            self.local_engine_process = None
            set_status_message(gb,"Local engine terminated",GREEN)

    #FIXME
    def start_slurm(self, gb,n, stime):
        self.lock.acquire()
        try:
            if(n < 0):
                set_status_message(gb,"Invalid number of engines to be started: " + str(n),YELLOW)
            else: 
                set_status_message(gb,"Starting SLURM engines",YELLOW)
                engine_path = "slurm_to_xslurm"
                cmd = "sbatch --parsable -J xslurm -t " + stime + " -p normal slurm_to_xslurm -a " + str(status.address) + " -p " + str(status.port)
                add_log_line(gb,"Starting engines using: " + cmd,GREEN)
                args = shlex.split(cmd)
            
                for i in range(n):
                    eid = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE).communicate()[0]
                    eid  = eid.strip()
                    if 'error' in eid or not eid:
                        add_log_line(gb, 'Error during SLURM submission.', RED)
                        return
                    self.engine_by_clusterid[eid] = None
                time.sleep(3)
                set_status_message(gb,"SLURM engines queued",GREEN)
                add_log_line(gb,str(n) + " SLURM engines queued",MAGENTA)
        finally:
            self.lock.release()

    #FIXME: jobs running removal, priority of cancelling less active nodes
    def stop_slurm(self, gb,n):
        self.lock.acquire()
        try:
            if(len(self.get_cluster_engines()) == 0 and n != 0):
                set_status_message(gb,"No SLURM engines running",YELLOW)
            elif(n < 0 or n > len(self.get_cluster_engines())):
                set_status_message(gb,"Invalid number of engines to be stopped: " + str(n),YELLOW)
            else:
                set_status_message(gb,"Stopping SLURM engines",YELLOW)

                stop_engines = []
                for cid, engine in self.engine_by_clusterid.items():
                    if engine is None:
                        stop_engines.append(cid)
               
                #stop non_running jobs
                stop_engines.sort()
                stop_engines = stop_engines[::-1] #cancel youngest non-running engine
                
                #stop idle jbos
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in self.engine_by_clusterid.items():
                        if not engine is None and len(engine.jobs) == 0: 
                            stop_engines2.append(cid)
                    stop_engines2.sort() #cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2

                #stop oldest jobs
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in self.engine_by_clusterid.items():
                        if not cid in stop_engines:
                            stop_engines2.append(cid)
                    stop_engines2.sort() #cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2

                stop_engines = stop_engines[:n]
                
                set_progress_bar(gb,0.0)
                for i,eid in enumerate(stop_engines):
                    cmd = "scancel " + eid
                    args = shlex.split(cmd)
                    add_log_line(gb, "Cancelling using command: " + cmd, CYAN)
                    p = Popen(args,stdout=subprocess.PIPE,stderr=subprocess.PIPE)
                    log_output(p.stdout,gb,WHITE)
                    log_output(p.stderr,gb,RED)
                    p.wait()

                    del self.engine_by_clusterid[eid] #FIXME: what to do with the jobs?
                    set_progress_bar(gb,float(i+1)/float(n))
                
                time.sleep(2)
                stop_progress_bar(gb)
                set_status_message(gb,"SLURM engines stopped",GREEN)
                add_log_line(gb,str(n) + " SLURM engines stopped",MAGENTA)
        finally: 
            self.lock.release()

   

    def stop_all(self):
        if len(self.get_cluster_engines()) > 0:
            add_log_line(gb,"Terminating SLURM engines.", RED)
            self.stop_slurm(gb, len(self.get_cluster_engines()))
        if self.check_local_engine():
            add_log_line(gb,"Terminating local engine.", RED)
            self.stop_local(gb)
    
    
    def register(self, myip,cores,totmem):
        self.lock.acquire()
        try:
            myid = myip.split('-')[0]
            if myid in self.engine_by_id:
                myid = 'DENIED'
                add_log_line(gb,"Secondary engine on " + myid + ' was denied',RED)
            else:
                e = Engine(myid, cores, totmem)
                self.engine_by_id[myid] = e
                if myid == status.address.split('-')[0]:
                    e.cluster_id = False

            
                totalgb = totmem / 1024.0
                average_use = totalgb / float(cores)
            
                add_log_line(gb,"Engine (" + str(cores) + "C:" + "{:1.1f}".format(average_use) + " GB) on " + myid + ' is now online',CYAN)
        
        except Exception, e:            
            add_log_line(gb,"Exception in register: " + str(e),RED)
        finally:
            self.lock.release()
        return myid

    def poll(self, myid, cpu_usage, mem_usage, mode):
        try:
            e = self.engine_by_id[myid]
            e.cpu_usage = cpu_usage
            e.mem_usage = mem_usage
            e.last_seen = time.time()
           
            commands = e.pending_commands
            if commands:
                self.lock.acquire()
                commands = e.pending_commands
                e.pending_commands = []
                self.lock.release()


        except KeyError:
            commands = []
            if mode == xslurm_shared.STOPPING:
                add_log_line(gb,"Poll for unknown engine id: " + myid + ". Engine is stopping.",RED)
                commands.append((xslurm_shared.STOP,None))
            else:
                add_log_line(gb,"Poll for unknown engine id: " + myid + ". Taking ownership.",RED)
                commands.append((xslurm_shared.REREGISTER,None))

        return commands
            



    def unregister(self, myid):
        self._unregister(myid)
        add_log_line(gb,"Engine with id " + myid + " unregistered",CYAN)
        return True

    def _unregister(self, myid):
        self.lock.acquire()
        try:
            e = self.engine_by_id[myid]
            del self.engine_by_id[myid]
            if not (e.cluster_id is None or e.cluster_id is False) and e.cluster_id in self.engine_by_clusterid:
                del self.engine_by_clusterid[myid]

            for rj in list(e.jobs):
                jobs.job_done(rj.jobid, -1, 'Node unregistered')

        except KeyError:
            add_log_line(gb,"Unregister request for unknown engine id: " + myid,RED)
        finally:
            self.lock.release()
    
    

engines = EngineManager()
unregister = lambda *args: engines.unregister(*args)
poll = lambda *args: engines.poll(*args)
register = lambda *args: engines.register(*args)

def thread_check_commands():
    while 1:
        
        engines.lock.acquire()
        try:
            engines._check_cluster_engines()
            all_engines = engines.engine_by_id.values()
        finally:
            engines.lock.release()
        
        now = time.time()
        for engine in all_engines:
            if now - engine.last_seen > TIMEOUT:
                engines._unregister(engine.engine_id)
                add_log_line(gb,"Engine with id " + engine.engine_id + " timed out",RED)
        
        used_cpus = 0
        used_mem = 0
        total_cpus = 0
        total_mem = 0
        max_mem_usage = 0.0
        max_cpu_usage = 0.0
        for engine in all_engines:
            total_cpus += engine.cores
            total_mem += engine.totmem / (1024.0)

            used_cpus = engine.cores * (engine.cpu_usage/100.0)
            used_mem = (engine.totmem / 1024.0) * (engine.mem_usage/100.0)

            max_mem_usage = max(max_mem_usage, engine.mem_usage/100.0)
            max_cpu_usage = max(max_cpu_usage, engine.cpu_usage/100.0)

        status.used_cpus = used_cpus
        status.total_cpus = total_cpus
        status.max_cpu_uasage = max_cpu_usage
        status.max_mem_usage = max_mem_usage
        status.total_mem = total_mem
        status.total_cpus = total_cpus

        time.sleep(5)



def thread_start_manager_server(port):
    server = SimpleXMLRPCServer(("", int(port)), logRequests = False, allow_none=True)
    server.register_function(register, "register")
    server.register_function(unregister, "unregister")
    server.register_function(poll, "poll")
    server.register_function(request_job, "request_job")
    server.register_function(job_finished, "job_finished")
    Servers.manager_server = server
    server.serve_forever()


def start_server(port):
    t = threading.Thread(target=thread_start_manager_server,args=(port,))
    t.daemon=True
    t.start()
    
    t = threading.Thread(target=thread_start_job_server,args=(port+1,))
    t.daemon=True
    t.start()

    t = threading.Thread(target=thread_check_commands)
    t.daemon=True
    t.start()


#create a bordered window
def border_win(parent,height,width,x,y,**kwargs):
    if(not 'notop' in kwargs):
        parent.hline(x,y,'.',width)
        height -= 1
        x += 1
    if(not 'nobottom' in kwargs):
        parent.hline(x+height-1,y,'.',width)
        height -= 1
    if(not 'noleft' in kwargs):
        parent.vline(x,y,'.',height)
        width -= 1
        y += 1
    if(not 'noright' in kwargs):
        parent.vline(x,y+width-1,'.',height)
        width -= 1
    w = parent.subwin(height, width,x,y)
    w.noutrefresh()
    parent.refresh()
    return w


#threaded output of a file to the log window
def thread_log_output(file, gb, color, filter):
    while 1 :
        line = file.readline()
        if (not line):
            break
         
        if (filter and len([f for f in filter if f in line]) > 0) :
            pass
        else:    
            add_log_line(gb,line,color)

def log_output(file, gb, color = 0, filter = []):
    t = thread.start_new_thread(thread_log_output, (file, gb, color, filter))
    return t



def get_stat_str(stats):
    cpu,memp,mempmax,mem,memmax = stats
    if(cpu < 0.50 or mempmax > 0.90):
        color = RED
    elif(cpu < 0.80 or mempmax > 0.70):
        color = YELLOW
    else:
        color = GREEN

    descr = "(" +  '{:.2%}'.format(cpu) + " CPU, " + \
            '{:.2%}'.format(memp) + "/" + '{:.2%}'.format(mempmax)  + "/" + \
            '{}'.format(int(mem)) + "MB MEM)"
    return descr,color

#thread display status
def thread_display_status(gb,status):
    while not status.stop_status_display:
        local_running = engines.check_local_engine()
        nqueue = engines.cluster_queued_engines
        nrunning = engines.cluster_running_engines
        njobs, nrunning = jobs.get_job_stats()
        
        gb.lock.acquire()
        try:
            if(gb.wcom_status and gb.wcom_status.getmaxyx()[1] > 50):
                sizey = gb.wcom_status.getmaxyx()[1]
                gb.wcom_status.erase()
                gb.wcom_status.addstr(1,1,"Jobs: ")
                gb.wcom_status.addstr(1,18,"%d/%d" % (nrunning, njobs),curses.color_pair(GREEN))
                
                
                gb.wcom_status.addstr(3,1,"Local engine: ")
                if not local_running:
                   gb.wcom_status.addstr(3,18,"NOT RUNNING",curses.color_pair(YELLOW))
                else:
                   gb.wcom_status.addstr(3,18,"RUNNING",curses.color_pair(GREEN))

                gb.wcom_status.addstr(5,1,"SLURM engines: ")
                nnodes = nrunning + nqueue

                
                if nqueue == 0:
                    color = GREEN
                else:
                    color = YELLOW
                gb.wcom_status.addstr(5,18,"%d/%d" % (nrunning,nnodes),curses.color_pair(color))

                
                gb.wcom_status.refresh()
        except Exception,e:
            add_log_line(gb,"Exception in display thread: " + str(e), RED)
        finally:
            gb.lock.release()
        time.sleep(1)

def start_status_display(gb,status):
    status.stop_status_display = False
    t = threading.Thread(target=thread_display_status,args=(gb,status))
    t.daemon=True
    t.start()
    return t

def stop_status_display(status,t):
    set_status_message(gb, "Waiting for status thread to terminate..", YELLOW)
    status.stop_status_display = True
    t.join()

#initialize windows/lock, etc.
def build_windows(gb):
    if(not gb.lock): 
        gb.lock = threading.RLock()
    gb.lock.acquire()
    try:
        
        #clear screen in curses does not work somehow. 
        print "\033[2J"

        gb.scr.erase()
        gb.scr.refresh()
        curses.start_color()
        # Invisibility mode of the cursor may not be supported by the terminal
        try :
            curses.curs_set(0)
        except :
            pass
        
        #curses getmaxyx does not work for resize screens to smaller sizes. Use stty instead. 
        rows, columns = os.popen('stty size', 'r').read().split()       
        size = (int(rows), int(columns))

        com_win_rows = max(0,min(20,size[0]-2))
        status_win_rows = max(0,min(2,size[0] - com_win_rows))
        log_win_rows = max(0,size[0] - com_win_rows - status_win_rows)


        #init colors
        curses.init_pair(RED,curses.COLOR_RED,curses.COLOR_BLACK)
        curses.init_pair(GREEN,curses.COLOR_GREEN,curses.COLOR_BLACK)
        curses.init_pair(YELLOW,curses.COLOR_YELLOW,curses.COLOR_BLACK)
        curses.init_pair(MAGENTA,curses.COLOR_MAGENTA,curses.COLOR_BLACK)
        curses.init_pair(BLUE,curses.COLOR_BLUE,curses.COLOR_BLACK)
        curses.init_pair(CYAN,curses.COLOR_CYAN,curses.COLOR_BLACK)
        
        #log window
        if(log_win_rows > 2):
            gb.wlog = border_win(gb.scr,log_win_rows,size[1],0,0)
            gb.wlog_size = gb.wlog.getmaxyx()
            gb.wlog_currow = 0
            gb.wlog.setscrreg(0,gb.wlog_size[0]-1)
            gb.wlog.idlok(True)
            gb.wlog.scrollok(True)
            gb.wlog.refresh()
        else:
            gb.wlog = None

        #com window
        if(com_win_rows > 10):
            com_win_cols = size[1] / 2
            if(com_win_cols > 55):
                comlog_win_cols = size[1] - 55
                com_win_cols = 55
                gb.wcom_status = border_win(gb.scr,com_win_rows,comlog_win_cols,log_win_rows,com_win_cols,notop=True,noleft=True)
                gb.wcom_status.refresh()
            else:
                com_win_cols = size[1]
                gb.wcom_status = None
        
            gb.wcom = border_win(gb.scr,com_win_rows,com_win_cols,log_win_rows,0,notop=True)
            gb.wcom.refresh()
        else:
            gb.wcom = None
            gb.wcom_status = None
        
        #status window
        if(status_win_rows == 2):
            gb.wstatus = gb.scr.subwin(status_win_rows,size[1],log_win_rows + com_win_rows,0)
            gb.wstatus.refresh()
        else:
            gb.wstatus = None

        init_commands(gb)
        gb.scr.refresh()
    except:
        pass
    finally:
        gb.lock.release()    

#add line to log windows
def add_log_line(gb,line,color=0):
    gb.lock.acquire()
    try:
        if(line and line[-1] == '\n'):
            gb.log_file.write(line)
        else:
            gb.log_file.write(line + '\n')
        if(gb.wlog):
            try:
                line = str(line.decode('ascii'))
            except UnicodeDecodeError:
                line = "Invalid line format (unicode) encountered"
                color = RED
            line = line[:(gb.wlog_size[1] - 2)]
            if(line and line[-1] == '\n'): line = line[:-1]
            if(gb.wlog_currow < gb.wlog_size[0]):
                gb.wlog.addstr(gb.wlog_currow,1,line,curses.color_pair(color))
                gb.wlog_currow += 1
            else:
                gb.wlog.scroll()
                gb.wlog.addstr(gb.wlog_size[0] - 1,1,line,curses.color_pair(color))
            gb.wlog.refresh()
    except Exception, e:
        print e
    finally:
        gb.lock.release()

#set status message iin status window
def set_status_message(gb, line, color = 0):
    gb.lock.acquire()
    try:
        if (gb.wstatus) :
            gb.wstatus.addstr(0,0,line,curses.color_pair(color))
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

def set_progress_bar(gb,s):
    gb.lock.acquire()
    try:
        if(gb.wstatus):
            if(s < 0.0 or s > 1.0):
                add_log_line(gb, "Progress bar value out of range: " + str(s),RED)
            else:
                size = gb.wstatus.getmaxyx()
                total_i = size[1] - 3
                full_i = int(s * total_i)
                rem_i = total_i - full_i
                line = "[" + ('#' * full_i) + (' ' * rem_i) + "]"
                gb.wstatus.addstr(1,0,line,curses.color_pair(MAGENTA))
                gb.wstatus.clrtoeol()
                gb.wstatus.refresh()
    except Exception,e:
        add_log_line(gb,"Exception in set_progress_bar: " + str(e),RED)
    finally:
        gb.lock.release()

def stop_progress_bar(gb):
    gb.lock.acquire()
    try:
        if(gb.wstatus):
            gb.wstatus.addstr(1,0,"")
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

#print commands in command windows
def init_commands(gb):
    gb.lock.acquire()
    try:
        if not gb.wcom is None:
            gb.wcom.addstr(1,1,"Start/stop local engine: s/x")
            gb.wcom.addstr(2,1,"Start/stop SLURM engines: d/c [n]")
            #gb.wcom.addstr(5,1,"Autogrow SLURM engines: g")
            #gb.wcom.addstr(6,1,"Autocancel SLURM engines: h")
            gb.wcom.addstr(9,1,"Shutdown: q")
            gb.wcom.refresh()
    except Exception, e:
        pass
    finally:
        gb.lock.release()

#enter command in command window
def enter_command(gb,argument="Command: "):
    if gb.wcom is None:
        return
    gb.lock.acquire()
    try:
        command = ""
        size = gb.wcom.getmaxyx()
        x = size[0] - 1
        gb.wcom.addstr(x,1,argument)
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
        while 1:
            gb.lock.release()
            c = gb.scr.getch()
            gb.lock.acquire()
            if c == curses.ascii.LF:
                break
            elif c == curses.ascii.DEL:
                command = command[:-1]
            else:
                if(c > 256):
                    set_status_message(gb,"Keycode not supported: " + str(c))
                else:
                    command += chr(c)
            gb.wcom.addstr(x,1,argument + command[:(size[1] - len(argument) - 2)])
            gb.wcom.clrtoeol()
            gb.wcom.refresh()

        gb.wcom.addstr(x,1,"")
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
    finally:
        gb.lock.release()
    return command

def get_number(gb,argument,default=0):
    cmd = enter_command(gb,argument)
    if(not cmd):
        n = default
    else:
        try:
            n = int(cmd)
        except ValueError:
            set_status_message(gb,"Number of engines should be a number",YELLOW)
            n = None
    return n







def main(scr, *args, **kwds):
    address = xslurm_shared.get_hostname()
    port = xslurm_shared.port

    status.set_address(address,port)
    
    gb.scr = scr
    gb.log_file = open('cluster.log','w')

    build_windows(gb)
    t = start_status_display(gb,status)
    start_server(port)
   

    #workaround for bug in curses: no resize messages
    os.environ['LINES']="blah"
    del os.environ['LINES']
    os.environ['COLUMNS']="blah"
    del os.environ['COLUMNS']

    try:
        while 1:
            c = gb.scr.getch()
            if c == curses.KEY_RESIZE:
                build_windows(gb)
            elif c == ord('q'):  #QUIT
                if engines.check_local_engine() or len(engines.get_cluster_engines()) > 0:
                    ans = enter_command(gb, "Are you SURE? Engines are STOPPED! (yes/no): ")
                else:
                    ans = enter_command(gb, "Are you sure? (yes/no): ")
                if ans == 'yes':
                    if engines.check_local_engine() or len(engines.get_cluster_engines()) > 0:
                        engines.stop_all()
                    stop_status_display(status,t)
                    gb.scr.refresh()
                    break
            elif c == ord('s'):  #START local engine
                cpus = get_number(gb, "Number of cpus (def=23): ", 23)
                mem = get_number(gb, "Maximal memory in GB (def=62): ", 62)
                if(not cpus is None and not mem is None):
                    engines.start_local(gb,cpus,mem * 1024.0)

            elif c == ord('x'):  #STOP local engines
                engines.stop_local(gb)

            elif c == ord('d'):  #START SLURM engines
                cmd = get_number(gb, "Number of engines (def=1): ",1)
                runtime = enter_command(gb, "Enter time (default=5-0:0:0): ")
                if runtime == "":
                    runtime = '5-0:0:0'
                if(not cmd is None):
                    engines.start_slurm(gb,cmd, runtime)

            elif c == ord('c'):  #STOP PBS engines
                cmd = get_number(gb, "Number of engines (def=all): ",len(engines.get_cluster_engines()))
                if(not cmd is None):
                    engines.stop_slurm(gb,cmd)

    	    elif c == ord('u') :  # Update ncurses screen
                gb.scr.refresh()
    except Exception:
        add_log_line(gb,"EXCEPTION!!! EMERGENCY SHUTDOWN IN PROGRESS!",RED)
        try:
            engines.stop_all()
            stop_status_display(status,t)
        except Exception:
            pass
        raise
        
    time.sleep(2) #wait for all paint ops to finish and engines to terminate
curses.wrapper(main)
